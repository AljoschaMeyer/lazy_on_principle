% https://2024.splashcon.org/track/splash-2024-Onward-papers#Call-for-Papers
% 13 Pages, excluding bibliography

\documentclass[sigplan,screen,10pt,review]{acmart}
% \documentclass[11pt,conference]{IEEEtran}

\input{libs.tex}
\usepackage{macros}

\title{Strict Principles for Lazy Sequences}
% \title{Principled Lazy Sequences}

\author{Aljoscha Meyer}
\affiliation{%
  \institution{Technical University Berlin}
  \city{Berlin}
  \country{Germany}
}
\email{research@aljoscha-meyer.de}

\begin{document}

\begin{abstract}
Many common programming tasks, such as networking, are conceptually about lazily working with sequences of unknown length. There are plenty of APIs to choose from --- stream and sink, reader and writer, iterator and oops-missing-counterpart. But these APIs typically vary between languages or libraries. Even within a single ecosystem, there often are inconsistencies between the processing models the different APIs induce.

We argue that a unified design is possible. We aim to provide a starting point for future language and library designers, as well as raise several interesting research questions that arise from taking a principled look at lazy sequences.
\end{abstract}

\maketitle

\section{Introduction}\label{introduction}

When sequences of data become too large to fit into memory at once, programs need to process them lazily. From the humble iterator to asynchronous APIs for streams and sinks with error handling and buffering, every language needs libraries for working with lazy sequences.

For such a fundamental, conceptually simple, and language-agnostic problem, one might expect a principled, unified solution that programming language designers and library authors can turn to and implement in their language of choice.

But the opposite is the case. Learning a new programming language implies learning yet another, slightly (or not so slightly) different set of APIs for working with sequences. Even within a single language, there are often competing libraries --- \cref{wtfjs} lists some thirty popular Javascript libraries alone.

Starting from ``\textit{Which} abstraction is the best?'', we quickly moved to ``\textit{Is} there a best abstraction?'', and then to the more constructive ``\textit{What} would make an abstraction the best?''. In this essay, we present our answers to these questions. In a nutshell:

\begin{enumerate}
    \item Abstractions for working with lazy sequences in the wild are ad-hoc designs.
    \item We propose a principled way of evaluating them.
    \item No prior abstractions satisfy all evaluation criteria.
    \item We develop abstractions that do.
    \item \sout{Everybody everywhere should use our abstractions without further reflection.}
\end{enumerate}

We further argue that common designs are needlessly inconsistent.

To give a concrete example: in Rust\footnote{Rust is simply our go-to programming language. We are not aware of any mainstream language that is significantly more consistent in its APIs.}, the (de-facto standard) \texttt{Stream} API\footnote{\url{https://docs.rs/futures/0.3.30/futures/stream/trait.Stream.html}} for receiving data from an asynchronous data stream has no notion of an end-of-stream and no notion of irrecoverable errors. The corresponding synchronous \texttt{Iterator} API\footnote{\url{https://doc.rust-lang.org/std/iter/trait.Iterator.html}} has a notion of an end of items and no notion of irrecoverable errors. The synchronous \texttt{Read} API\footnote{\url{https://doc.rust-lang.org/std/io/trait.Read.html}} for receiving many items from a data source with a single function call has both a notion for an end of data availability and for irrecoverable errors; items are hardcoded to bytes, and errors are hardcoded to a general-purpose I/O error type. The asynchronous \texttt{Sink} API for processing a sequence of items\footnote{\url{https://docs.rs/futures/0.3.30/futures/sink/trait.Sink.html}} has both a notion of an end of processing (the \texttt{close} function) and a notion of irrecoverable errors. The synchronous counterpart does not exist. The synchronous \texttt{Write} API for processing many items into a sequence with a single function call\footnote{\url{https://doc.rust-lang.org/std/io/trait.Write.html}} has both a notion of closing and of irrecoverable errors.

Conceptually, these APIs only deal with two issues: lazily producing or consuming a sequence. Rust has language-level mechanisms for expressing specialization of APIs and subtyping relations between APIs. Yet each of these abstractions is defined in isolation, with fundamentally different choices of expressivity that make it cumbersome or impossible to fluidly convert between them.

Such a lack of consistency causes unnecessary education efforts (switching from synchronous to asynchronous sequence processing requires learning a completely different set of APIs), forces programmers to adopt inefficient code (raw bytes are not the only items for which bulk processing is more efficient than individual processing), and introduce a frustratings barrier to expressing a conceptual architecture of data flows and error handling as actual code.

Hence, the abstractions we propose emphasize consistency, and they build on top of each other. This sounds boring and obvious, and it \textit{should} be boring and obvious. Alas, it apparently is not.

To keep the scope manageable, we restrict our focus to the two simplemost ways of interacting with a (possibly infinite) sequence: \textit{consuming} a sequence item by item, or \textit{producing} a sequence item by item. Both modes of interaction are of great practical interest, they correspond, for example, to reading and writing bytes over a TCP connection. We do not consider more complex settings such as random access in our main treatment.

We assume a strictly evaluated language. This makes explicit the design elements that enable laziness.

\subsection{Evaluating Sequence APIs}

Equipped with a vague notion of wanting to ``lazily consume or produce sequences'', how can we do better than simply trying to find a design that satisfies all use-cases we can come up with? In mathematics, one would define a set of criteria that a solution should satisfy, in a way that makes no assumptions about any possible solutions themselves.

For example, a mathematician might want to work with numbers ``with no gaps in-between'' (i.e., the real numbers). They might formalize this intuitive notion as a minimal, infinite, complete ordered field. Any candidate construction (say, the Dedekind cuts of rational numbers), can now be objectively measured against the requirements. As an added bonus, it turns out that all constructions satisfying the abstract requirements are isomorphic. Some constructions might be more convenient than others in certain settings, but ultimately, they are all interchangeable.

This approach of construction-independent axiomization is the only way we can reasonably expect to bring clarity to the proliferation of competing API designs for lazy sequences.

Sadly, we could not find an airtight mathematical formalism to capture our problem space. The criteria we now present leave gaps that must be filled by argumentation rather than proof, the API design still remains part art as much as science. We nevertheless think that both our approach and our results are novel --- and useful.

The criteria by which we shall evaluate lazy sequence abstractions are \textit{minimality}, \textit{symmetry}, and \textit{expressivity}.

\textbf{Minimality} asks that no aspect of the API design can be expressed through other aspects of the design. Removing any feature impacts what can be expressed.

% \textbf{Universality} asks that the same fundamental design works irrespective of whether processing is synchronous or asynchronous, buffered or unbuffered, etc. We want a solution that is applicable indepedently of such implementation details.

\textbf{Symmetry} asks that reading and writing data should be dual. The two intuitive notions of producing and consuming a sequence item by item are fully symmetric and sit on the same level of abstraction. Any API design that introduces an imbalance between the two must either be contaminated with incidental complexity, or it must be missing functionality for one of the two access modes.

\textbf{Expressivity} asks that the API design is powerful enough to get the job done, but also no more powerful than necessary. This is by far the most vague of our criteria, because we cannot simply equate more expressivity with a better design. We \textit{can}, however, draw on the theory of formal languages to categorize the classes of sequences whose consumption of production can be described by an API. Some of these classes are more natural candidates than others.

Of these criteria, minimality is arguably the least controversial. Symmetry turns out to be the one we generally find the most neglected in the wild.. Expressivity might have the weakest definition, but turns out to be rather unproblematic: real-world constraints on the APIs lead to a level of expressivity that also has a convincing formal counterpart --- the $\omega$-regular languages (see \cref{evaluating_ours} for details) --- making us quite confident about the appropriate level of expressivity.

To obtain a first indicator for an appropriate level of expressivity, we examine the world of non-lazy sequences, i.e., sequences that can be fully stored in memory.

\subsection{Case Study: Strict Sequences}\label{strict_sequences}

Representing sequences in memory can be done in such a natural way that we have never seen any explicit discussion. We shall assume a typical type system with product types (denoted $(S, T)$), sum types (denoted $S + T$), and homogeneous array types (denoted $[T]$).

Let $T$ be a type, then $T$ is also the type of a sequence of exactly one item of type $T$. Now, let $S$ and $T$ be types of sequences. Then $(S, T)$ denotes the concatenations of sequences of type $S$ and sequences of type $T$, $S + T$ denotes the sequences either of type $S$ or $T$, and $[T]$ denotes the concatenations of arbitrarily (but finitely) many sequences of type $T$. None of this is particularly surprising, we basically just stated that algebraic data types and array types allow you to lay out data sequentially in memory.

Slightly more interesting is the blatant isomorphism to regular expressions. Each of the ``sequence combinators'' corresponds to an operator to construct regular expressions; the empty type and the unit type correspond to the neutral elements of the choice and concatenation operator respectively.

This is useful for making our expressivity requirement for lazy sequence APIs more precise: if the natural representation of strict sequences admits exactly the regular languages, then the regular languages are also the natural candidate level of expressivity for lazy APIs.

Unlike strict sequences that have to fit into finite memory, lazy sequences can be of infinite length. The natural generalization of the regular languages to infinite strings are the $\omega$-regular languages. Hence, this is the level of expressivity we want to see in lazy APIs.

The strict case also neatly validates the design goals of minimality and symmetry. Removing any combinator leads to a strictly less expressive class of languages, and every operator comes both with a way of building up values and with a way of accessing values.

By generalizing the strict case to the lazy case, we can make our requirement of expressivity more precise, leading us to our final set of requirements: We want APIs for lazily producing or consuming sequences an item at a time, such that there is a one-to-one mapping between API instances and $\omega$-regular languages, no aspect of the APIs can be removed without loosing this one-to-one mapping, and there is full symmetry between consumption and production of sequences. Still not entirely formal, but close enough to meaningfully evaluate and design APIs.


\section{Evaluating Abstractions}\label{evaluating_others}

We now start by introducing a notation for API designs. We then express several APIs we encountered in the wild in our notation, in order to build intuition, demonstrate sufficient notational expressivity, and to highlight typical violations of our design goals in popular APIs. We do not aim for an exhaustive survey of APIs, we merely select some examples to illustrate our points.

In the following, we use uppercase letters as type variables. $(S, T)$ denotes the product type of types $S$ and $T$ (intuitively, the cartesian product of $S$ and $T$), and $()$ denotes the unit type (intuitively, the type with only a single value). $S | T$ denotes the sum type of $S$ and $T$ (intuitively, the disjoint union of $S$ and $T$), and $!$ denotes the empty type (the type that admits no values). Finally, we write $S \rightarrow T$ for the type of (pure) functions with an argument of type $S$ and a return value of type $T$. Note we take a purely functional approach here: a function does not mutate its argument, it simply produces a new value.

We specify an API as a list of named types (typically functions). Each API can quantify type variables that can be used in its \textit{members}\footnote{More formally, this is a notation for ad-hoc polymorphism~\cite{wadler1989make} like Haskell's type classes, Java's interfaces, or Rust's traits.}. As an example, consider the following API:

\begin{lstlisting}[language=Python]
API Iterator<P, I>
    next: P -> (I, P) | ()
\end{lstlisting}

This pseudo-type fragment states that in order to obtain a concrete \texttt{Iterator}, one needs two types: a type $P$ (\textbf{P}roducer) and a type $I$ (\textbf{I}tem). These types have to be related through existence of a function \texttt{next}, which maps a producer to either an item and and a new producer, or to a value that signifies that no further items can be produced.

To consume this iterator, one would repeatedly call \texttt{next} on the producer returned from the prior call of \texttt{next}, until a call returns $()$.

A concrete example of such an iterator are the homogenous arrays of $I$s as producers of items of type $I$s; \texttt{next} returns $()$ for the empty array, otherwise it returns the first item in the array and the array obtained by removing the first item.

This API is completely stateless, we never mutate any $P$. In an imperative programming language, one would typically use a function that takes a reference to a $P$ and returns either an $I$ or $()$, and then make all implementors pinky-swear to not invoke the function with a $P$ that has returned $()$ previously.

We prefer the purely functional notation, because it can express the pinky-swearing API contract on the type level. But all our designs can easily be translated into an imperative, stateful setting. The other way around, by converting stateful references into input values and output values, we can represent APIs from imperative languages in our notation. For example, this \texttt{Iterator} API captures the semantics of commonly used iterator APIs such as those of Python\footnote{\url{https://wiki.python.org/moin/Iterator}} or Rust\footnote{\url{https://doc.rust-lang.org/std/iter/trait.Iterator.html}}. It handily abstracts over the fact that Rust has actual sum types, whereas Python signals the end of iteration with an exception.

% Without devoting too much space to it, we want to point out that even for such a simple special case of sequence processing as iteration --- i.e., synchronous production of finite sequences without buffering or error handling --- there are several, non-isomorphic approaches in the wild. Both Java\footnote{\url{https://docs.oracle.com/javase/8/docs/api/java/util/Iterator.html}} and Javascript\footnote{\url{https://tc39.es/ecma262/multipage/control-abstraction-objects.html\#sec-iterator-interface}} use APIs with slightly different properties; they differ in how early the code interacting with the iterator knows that no further items will be produced.

Given the symmetry between producing and consuming data, the virtual non-existence of (synchronous, non-error-handling) APIs for consuming data step by step is jarring. Java has an \texttt{Appendable} interface\footnote{\url{https://docs.oracle.com/javase/8/docs/api/java/lang/Appendable.html}}, but it is specialized to characters, and has a much less prominent role than the \texttt{Iterator} interface. Clojure complects both production and consumption into a single \texttt{Sequence} interface\footnote{\url{https://clojure.org/reference/sequences\#_the_seq_interface}}.

Several languages do give the opposite of iterators a prominent role via specialzed loop statements that consume an iterator item by item. Handling sequence production via first-class values while turning sequence consumption into a purely syntactic component with no run-time presence massively breaks symmetry from a high level design perspective.

Moving beyond synchronous iterators, \textit{asynchronous} producers --- often called \textit{streams} --- typically come with buffering and error handling. We shall abstract over\footnote{The seasoned Haskell-affectionado will immediately see that parameterizing our presentation over a monad for effectful computation~\cite{wadler1995monads} would restore rigorous reasoning to our handwavy act of abstraction. We posit that to the readers who are already familiar with effect-management through monads, filling in the blanks is an easy exercise. To those readers who are not famliar with this technique --- i.e., the vast majority of practicioners we would like to reach --- obscuring our presentation behind higher-kinded type constructors poses an unnecessary barrier to access. Aside from a recent example of a monad for asynchronous effects~\cite{zhao2020asynchronous}, the interested reader might also want to look at asynchronous \textit{algebraic} effects~\cite{leijen2017structured}\cite{ahman2021asynchronous}.} both buffering and asynchronicity, as they do not influence matters of minimality, symmetry, or expressiveness regarding formal languages. We believe that designing a solid API irrespective of buffering and asynchronicity and then adding them in a way that is idiomatic for the programming language in question leads to clearer designs. In particular, we see no reason why synchronous and asynchronous sequence APIs in the same language should not be completely analogous.

An example where the asynchronous producer API is equivalent to the synchronous iterator API is --- as of writing --- Rust\footnote{\url{https://docs.rs/futures/0.3.30/futures/stream/trait.Stream.html}}. This is rather atypical, because asynchronous APIs, which are often motivated by networking, usually emphasize error handling. In the iterator API, one can use a sum type of actual items and an error type as the type $I$, but on the type level, it remains possible to continue iterating after an error. The API is accurate for recoverable errors only.

A different example is Swift\footnote{\url{https://developer.apple.com/documentation/swift/asyncsequence}}, which superficially appears to offer an equivalent API, but the language-level feature of throwing exceptions allows the ability to express irrecoverable errors\footnote{\url{https://developer.apple.com/documentation/swift/asyncthrowingstream}}. In our notation, the Swift API is:

\begin{lstlisting}[language=Python]
API FallibleIterator<P, I, E>
    next: P -> (I, P) | () | E
\end{lstlisting}

This particular API design violates minimality: removing the option of returning the unit type would leave the degree of expressivity unchanged, since one could always instantiate $E$ as a sum type of the unit type and an actual error type. It might be tempting to argue that the Swift API communicates intent more clearly, and allows for nicer library functions for working with streams. But these conveniences and specializations could just as well be implemented as special cases of the more general, underlying pattern. Rust nicely demonstrates this by offering a host of functions\footnote{\url{https://docs.rs/futures/latest/futures/stream/trait.TryStreamExt.html}} for working with streams whose items are a sum type of actual items and error values. Offering a special case as the most fundamental API, like Swift does, unnecessarily reduces expressivity.

Another school of APIs defines asynchronous producers in terms of all the ways in which one would commonly interact with them: mapping, reducing, piping into a consumer, etc. Examples include Java\footnote{\url{https://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html}} or Dart\footnote{\url{https://api.dart.dev/stable/3.3.0/dart-async/Stream-class.html}}. Such designs are not concerned with minimality at all. Our preferred approach is to find the minimal interface that that allows expressing all these higher-level functions on top.

The story for asynchronous \textit{consumers} is usually highly asymmetrical again. Swift, for example, has asynchronous for loops to consume streams. Rust offers a \textit{sink} abstraction\footnote{\url{https://docs.rs/futures/0.3.30/futures/sink/trait.Sink.html}} as the opposite of a stream, but the designs are highly asymmetric --- the sink abstraction requires four functions to the stream abstraction's one function.

% Universality and specialization to obtain iterators.


\section{A Principled Design}\label{main_design}

From this backdrop of rather inconsistent designs, we now present our alternatives. We derive --- in tandem --- APIs for producing and consuming sequences, guided by minimality, symmetry, and expressivity (\cref{derive}). Next, we argue that the designs are indeed sufficiently symmetric and of appropriate expressiveness (\cref{evaluating_ours}) --- if our arguments are not fully formal, they are at least \textit{formalizable}. Finally, we sketch some consequences the designs could have for language-level features such as generators or for loops (\cref{syntax}).

\subsection{Deriving Our Design}\label{derive}

To derive a principled design step by step, we start with simplemost producer API: a producer that emits an infinite stream of items of the same type.

\begin{lstlisting}[language=Python]
API InfiniteProducer<P, I>
    produce: P -> (I, P)
\end{lstlisting}

An iterator, in contrast, expresses a \textit{finite} stream of items, by making the result of a sum type with a unit type otion to signal termination. We can easily abstract over both through a simple realization: we can rewrite the \texttt{produce} function of the \texttt{InfiniteProducer} as a sum with the empty type, without changing the semantics at all (it is impossible to provide an instance of the empty type, the empty type is the neutral element of type sums):

\begin{lstlisting}[language=Python]
API AlsoAnInfiniteProducer<P, I>
    produce: P -> (I, P) | !
\end{lstlisting}

Now, the infinite producer and the finite producer have the exact same form, and we can introduce a type parameter for the summand to express either:

\begin{lstlisting}[language=Python]
API Producer<P, I, F>
    produce: P -> (I, P) | F
\end{lstlisting}

Setting the type parameter \texttt{F} (for \textit{\textbf{f}inal item}) to \texttt{!} or \texttt{()} yields the infinite streams and the finite streams over a single type of items, respectively.

Another natural choice for \texttt{F} are irrecoverable error types; most APIs with this design denote the type parameter as a type of errors explicitly. This denotation obscures how the same abstraction can also represent iterators or infinite streams, however.

Moreover, it obscures that \texttt{F} might be another producer itself, with which to continue production. Through this use of the API, we can effectively concatenate any producer after any finite producer. This usage is the cornerstone of achieving the expressivity of the $\omega$-regular languages, and one we have not encountered in the wild at all.

To give a tangible example of how this degree of expressivity can be useful, consider a networking protocol that proceeds in stages: first a handshake for connection establishment, followed by an exchange of key-value pairs that signify the capabilities of an endpoint, followed by the application-level message exchange. With an API parameterized over arbitrary final values, you can implement each stage in a type-safe way, and then concatenate the stages both in execution and on the type-level. Traditional APIs force programmers to either lump the different kinds of messages (handshake, key-value pairs, application-level) into a single sum type, or to forego helpful typing altogether and operate on the level of bytes.

A symmetric consumer API should be one that can be given either an item of type \texttt{I} --- returning a new consumer value to continue the process --- or a final item of some type \texttt{F} --- without returning a consumer to continue with. Ideally, we should be able to mechanically derrive this API as a dual of the producer API. A tempting option is to ``flip all arrows'' and simply swap argument and return type of the \texttt{produce} function:

\begin{lstlisting}[language=Python]
API Recudorp<P, I, F>
    ecudorp: ((I, P) | F) -> P
\end{lstlisting}

We can clean this up by splitting the function of a sum type argument into two independent functions (both versions are equivalent), and giving more conventional names:

\begin{lstlisting}[language=Python]
API NotQuiteConsumer<C, I, F>
    consume: (I, C) -> C
    create: F -> C
\end{lstlisting}

Unfortunately, this does not give the kind of API we were hoping for. The \texttt{consume} function is appropriate, but the second function is not closing a consumer, but creates a consumer. A straightforward dual construction gives too strong of a reversal to yield an API suitable for practical use.

Instead of a fully dual construction, we instead derive a consumer API in steps analogous to those for deriving the \texttt{Producer} API. We start again with the consumers of an infinite sequence and the consumer of a finite sequence:

\begin{lstlisting}[language=Python]
API InfiniteConsumer<C, I>
    consume: (I, C) -> C

API FiniteConsumer<C, I>
    consume: (I, C) -> C
    close: C -> C
\end{lstlisting}

We can again introduce a type parameter for the final sequence item to unify the APIs; observe how using $!$ or $()$ for the parameter \texttt{F} in the following API yields results isomorphic to the \texttt{InfiniteConsumer} and \texttt{FiniteConsumer} APIs respectively:

\begin{lstlisting}[language=Python]
API Consumer<C, I, F>
    consume: (I, C) -> C
    close: (F, C) -> ()
\end{lstlisting}

This API is fully symmetric to the \texttt{Producer}: the consumer can consume exactly those sequences that a producer can produce. It is rather unusual in that we have never seen an API whose \texttt{close} function takes an argument in the wild.

Another unusual aspect is the inability of a consumer to report errors to its calling code. This is severe enough of a departure from typical APIs that we discuss this in more detail in \cref{communication_flow}. But first, we will argue that these \texttt{Producer} and \texttt{Consumer} APIs indeed fulfil our initial design requirements.

\subsection{Evaluating Our Design}\label{evaluating_ours}

Convincing arguments for the symmetry of the APIs are not immediately apparent; while we derived both APIs throuh analogous steps, they are not immediately dual in any obvious sense, and they even have a different number of functions. The closest we could come to a formal argument is to show that they compose in a satisfying way.

Composing a producer with a consumer amounts to piping the data that the producer produces into the consumer:

\begin{algorithmic}
\Require $P, C, I, F$ are types such that \texttt{Producer<P, I, F>} and \texttt{Consumer<C, I, F>}
\Procedure{pipe}{$p: P$, $c: C$}{$: ()$}
    \Loop
        \State $x \gets \Call{produce}{p}$
        \If{$x$ is of type $F$}
            \State \Call{close}{$x, c$}
            \State \texttt{return} $()$
        \Else
            \State $c \gets \Call{consume}{x.0}$
            \State $p \gets x.1$
        \EndIf
    \EndLoop
\EndProcedure
\end{algorithmic}

Notice that the \texttt{pipe} function returns the unit type, it leaks no information to the outside. More traditional APIs have no way to funnel the final (usually irrecoverable error) value of the producer to the consumer, so the pipe function has to return that value to the outside. Similarly, APIs that allow consumers to emit errors also have to forward such errors from the pipe function.

On a purely abstract level, composing to the unit type evokes the concept of an element and its inverse composing to an identity element. This seems as strong a formal notion of symmetry we can hope for, without \textit{actually} formalizing things.

More concretely, this means that a producer and a consumer together can fully describe a (sub-)program that processes a data stream. While traditional APIs also allow building up programs by combining and modifying producers and consumers, there always has to be ad-hoc glue code for handling errors, and for moving from one stage of processing to the next. Our APIs capture a fuller set of tasks of such programs. In principle, with an expressive library for constructing producers and consumers, it should be possible to describe much wider classes of programs by piping a single producer into a single consumer, with no glue code or ad-hoc error handling. In particular, progressing from one processing stage to the next can be done by having the producer emit a new producer as a final value, and the consumer pipe this value into an inner consumer in its \texttt{close} function.

We can make a similar argument for composing the other way around: it should be possible to create a pair of a consumer and a producer such that the producer produces everything that the consumer consumes, in the same order (an in-memory queue). This is, in some vague sense, the neutral element of transformation steps in a pipeline (we return to this concept in \cref{conducer}). Here again, we see the benefits of the \texttt{close} function taking an argument: we can map this argument directly to the final value to be emitted by \texttt{produce}.

Libraries without this feature usually only offer queues parameterized over a single item type. One common tasks where this becomes unnecessarily restrictive include adding intermediate queues for buffering that can also delay propagating an error to the queue's consumer until all items that were emitted before the error have been consumed. Another usecase is testing: if you want to test a component by replacing its input producer with a stub, you cannot use the normal in-memory queue as a replacement, because it cannot emit final values (i.e., errors, in most libraries) on demand.

Having argued that our design is indeed symmetric in a meaningful way, we turn to the question of expressivity. Our core argument rests on the observation that each \texttt{Producer} (or \texttt{Consumer}) defines a formal language over an alphabet of atomic types. More precisely, a \texttt{Producer<P, I, F>} emits an arbitrary number of repetitions of values of type $I$, followed by a single value of type $F$. In more traditional notation of a language as a set of strings, it denotes the set $\set{I}^{\ast} \circ \set{F}$.

Given this mapping from sequence APIs to languages, which class of languages do our APIs describe? We claim they --- in concert with sums, products, and functions --- describe the union of the regular and the $\omega$-regular languages.

The \textit{$\omega$-regular languages} over $\Sigma$ are the sets of infinite strings over $\Sigma$ that are either a concatenation of infinitely many words from the same regular language\footnote{We assume familiarity with regular languages, for an introduction see~\cite{hopcroft1969formal}, for example. Or do the sensible thing of searching for ``regular language'' on Wikipedia.} over $\Sigma$ (\textit{infinite iteration}), or the concatenation of a regular language and an $\omega$-regular language over $\Sigma$, or a choice between finitely many $\omega$-regular languages over $\Sigma$.

As already argued in \cref{strict_sequences}, sum types and product tyes correspond to choice and concatenation of regular expressions respectively. Unlike the strict case, we cannot rely on homogeneous arrays to act as the counterpart to the Kleene operator, but this appears to be where the \texttt{Producer} API comes in (everything applies analogously for the \texttt{Consumer} API): a \texttt{Producer<P, I, F>} can produce an arbitrary number of repetitions of \texttt{I}s, followed by a single \texttt{F}. In particular, \texttt{Producer<P, I, ()>} corresponds to the Kleene operator, and \texttt{Producer<P, I, !>} corresponds to infinite iteration.

Unfortunately, this simple perspective is not fully accurate. Product types as concatenation are too powerful for us: consider a product $(P_1, P_2)$, where $P_1$ is a \texttt{Producer<P, I, !>}. The corresponding language would be a concatenation with an $\omega$-language on the left, but this is explicitly ruled out by the definition of $\omega$-regular languages. Another facet of the same problem is that the type $(S, T)$ is not one that describes \textit{first} emitting an $S$ and \textit{then} a $T$, as it presents both simultaneously.

To solve this, we can restrict the set of well-formed sequence types we consider to pairs $(S, () \rightarrow T)$ for (sums of) non-repeated types $S$ and arbitrary types $T$, and \texttt{Producer<P, S, T>} for repeated types $S$. This removes the ability to express concatenations with an $\omega$-language as the left operand, and introduces the required indirection to express ``first $S$, then $T$'' (remember that we assume our functions to abstract over effects, so there might well be asynchronicity involved in obtaining the $T$ after processing the $S$).

We shall not dwell on this subtlety any further, because it does not affect our two main points: our API is expressive enough to decribe regular ($\omega$-)regular languages, whereas a more traditional API \textit{without} a dedicated type for the final item is \textit{not} expressive enough, resulting in an unjustified reduction in expressive power compared to representing strict sequences in memory.

\subsection{Communication Flow}\label{communication_flow}

As already mentioned, our proposed \texttt{Consumer} API has no way of emitting errors to its calling code, a design decision that appears to make it unsuitable for network programming, for example. More generally speaking, code interacting with a \textit{consumer} can pass information \textit{to} the consumer but cannot obtain any information \textit{from} the consumer; and conversely, code interacting with a \textit{producer} can obtain information \textit{from} the producer but cannot pass any information \textit{to} the producer.

This rigorous a restriction on communication flows evokes design choices such as the unidirectional communication primitives of security-focussed micro-kernels like seL4~\cite{murray2013sel4}, so there clearly is a place for such constrained APIs. But the consumer API does not seem appropriate as a general-purpose API.

Trying to add the missing communication flows raises some interesting questions. Should \texttt{consumer} return the next consumer \textit{and} another piece of information, or the next consumer \textit{or} another piece of information? What about \texttt{close} --- should it be able to return extra information, or not? How should symmetry be preserved --- does the \texttt{Producer} API require a \texttt{stop} function that lets the surrounding code communicate that (and why) \texttt{produce} will no longer be called? Should \texttt{produce} itself take a piece of information as input?

Our fairly principled approach of aiming for  minimal, symmetric, regular-languagy APIs provides no guidance here, as these communication flows are not part of the underlying problems. Any choices we need to make are essentially arbitrary.

We see two ways out of this problem. The simplemost solution is acceptance. When a programmer wishes to write data to a network through a consumer interface, they need a corresponding producer to emit any feedback such as connection failures. Considering that typical networking APIs use the same error type for reading and writing data, this doesn't seem too far-fetched. Then again, the difficulties in migrating from more typical APIs to this style of error handling are hard to estimate. An argumentative essay like this one cannot conclusively establish a result, we merely want to raise that accepting a consumer API without error reporting might be more feasible than it appears at first glance.

The other solution is to consider fallibility as an \textit{effect}. Just like the functions we use in our APIs might be asynchronous, they might also be fallible. Different programming languages can represent this in different ways: some could use exceptions, others could consistently use a \texttt{Result} type (a sum type of either the actual value of interest or an error value) --- the latter is a simple and classic example of monadic effect handling. We can keep using the same notation as before, but consider every function as possibly fallible.

To make explicit the transformation from our APIs to fallible versions, here is what the results look like for errors of type $E$, when neither relying of exceptions nor on explicitly abstracting over effects via monads:

\begin{lstlisting}[language=Python]
API FallibleConsumer<C, I, F, E>
    consume: (I, C) -> C | E
    close: (F, C) -> () | E

API FallibleProducer<P, I, F, E>
    produce: P -> (I, P) | F | E
\end{lstlisting}

It is interesting to consider that the \texttt{FallibleProducer} API appears asymmetric to the \texttt{FallibleConsumer} API without the context of errors-as-effects, as it does not mirror the communication flow of the consumer by having functions that take $E$s as arguments. The return type of \texttt{produce} also appears to violate minimality, as $E$ and $F$ could be combined into a single type parameter in principle. This invalidates our earlier criticism of the Swift-derived \texttt{FallibleIterator} API of \cref{evaluating_others}: a return type of \texttt{I | () | E} correctly separates the final item (hardcoded to type \texttt{()} in the Swift case) from the error effect of type $E$.

It took a few pages, but now we have a more nuanced view by which to understand the API designs encountered in the wild. This deeper understanding can hopefully serve to guide future API designs that otherwise would have to rely on gut-feeling and copying older designs.

We will continue our investigation with the \texttt{Producer} and \texttt{Consumer} APIs as they are, it is up to the reader to decide whether their functions should be fallible (and/or asynchronous, for that matter), or not.

\section{Working With Producers and Consumers}

Having settled on designs for \texttt{Producer} and \texttt{Consumer} APIs, we now turn to how they can or should should be used in practice. We note a powerful pattern composability in \cref{conducer}, muse about language-level support in \cref{syntax}, before turning to matters of efficiency in \cref{bulk}.

\subsection{Conducers}\label{conducer}

In \cref{evaluating_ours}, we briefly considered an in-memory queue: a pair of a consumer and a producer such that the producer emits exactly the item consumed by the consumer. We can consider such a pair as a single value that implements both the \texttt{Consumer} and the \texttt{Producer} API; we shall call such a value a \textit{naïve conducer} (portmanteau of \textit{con}sumer and pro\textit{ducer}). The \textit{naïveté} will become apparent once we go from intuitive notions of composability to actual implementation; for now we ask you to suspend some disbelieve and let the concept guide us to the more useful, \textit{actual} conducers.

Naïve conducers make an appealing foundation for constructing and composing producers and consumers. You can use a single naïve conducer definition to both obtain a new producer from a producer or a new consumer from a consumer. Consider the naïve queue conducer: composing a producer with the naïve conducer yields a new producer that buffers some number of items before emitting them. Composing the naïve conducer with a consumer yields a new consumer that buffers some number of items before consuming them in the inner consumer.

This dual-purpose usage constitutes a tangible advantage of being hellbent on symmetry. As a second example, consider a naïve conducer constructed from some function of type $S \rightarrow T$ that is a consumer for items of type $S$ and a producer for items of type $T$. This naïve \textit{map} conducer can both adapt the items emitted by a producer, or adapt the items accepted by a consumer.% Contrast this with designs that have to provide individual map adaptors for both producers and consumers.

Naïve conducers need not preserve a one-to-one mapping between consumed items and produced items. The common tasks of encoding and decoding values for transoprt can be captured elegantly by naïve conducers: a \textit{decoder} consumes items of some type $S$ (often, $S$ would be the type of bytes) and occasionally produces an item of some type $T$, an \textit{encoder} consumes items of some type $T$ and produces many items of some type $S$.

Unfortunately, none of this actually works. In order to, for example, compose a naïve conducer in front of a consumer, the \textit{consume} function of the resulting consumer would have to first call the \textit{consume} function of the naïve conducer. Then, it would need to correctly guess how many times to call the naïve conducer's \textit{produce} function, in order to feed the results to the inner consumer. A general-purpose composition routine can neither know how many items the inner consumer expects, nor how many items the naïve conducer can produce at any point in time.

One obvious solution is to explicitly manage metadata about which functions can and should be called at runtime, but this creates computational overhead. Another simple solution is to restrict naïve conducer to producing exactly one item per item they consume, but this severely restricts expressivity.

Toward a zero-overhead, expressive solution, we temporarily abandon the dual-usage intuition behind naïve conducers, and examine consumers and producers separately. We define a \textit{consumer adapter} as a function that maps an arbitrary consumer to another consumer, and a \textit{producer adapter} as a function that maps an arbitrary producer to another producer.

These adapters can implement the same functionality as naïve conducers in a way that actually works. Consider, for example, a consumer adapter for encoding items of type $S$ to many items of type $T$. The consumer adapter can produce a consumer that consumes an item of type $S$, computes the encoding, and calls the \textit{consume} function of the inner consumer once for each $T$ of the encoding. The corresponding producer adapter, when asked to produce a value of type $T$, asks the wrapped producer for value of type $S$, and computes the encoding. It then returns the first $T$ of the encoding and buffers the remaining encoding, to be admitted on subsequent calls to \textit{produce}. Only when the buffer has become empty does it request another item from the wrapped producer.

There is a large amount of overlap or symmetry between the encoding consumer adapter and the encoding producer adapter, note how both use the same procedure for the actual encoding, and both need to buffer the result in between subsequent calls to the wrapped consumer producer. We call a pair consumer and producer adapters that implements a naïve conducer an (actual) \textit{conducer}.

While such conducers are an interesting tool to reason about working with lazy sequences, they do not provide an immediate software engineering benefit: the two adapters need to be implemented independently. In the spirit of full symmetry, we now have to duplicate all implementation efforts.

To improve on this, we next take a look at how programming language syntax (or macros) to make it possible to write a single definition that then yields both adapters of a conducer. To do so, we first need to investigate dedicated syntax for producers and consumers separately.

% Note that naïve conducers compose nicely amongst themselves, you can chain naïve conducers to create arbitrarily complex processing pipelines. Plugging these pipelines between two leaf producers and consumers, say ones that provide read access and write access to a tcp connection, is a comparatively boring and inconsequential formality.

% Conducers eliminate the need to duplicate implementation efforts for producer adapters and consumer adapters. With a sufficient degree of symmetry but without an explicit concept of conducers, library authors might be tempted to avoid duplicated efforts by keeping one of the two abstractions --- typically the consumers --- deliberately plain and devoid of combinators, while offering a rich set of combinators for the other abstraction. Conducers offer an alternative to this asymmetric and ultimately completely arbitrary distinction: both producers \textit{and} consumers should be kept plain; rich combinators should be reserved for conducers.

% As a final thought, we want to mention that you can view conducers as an overarching abstraction for both producers and consumers:

% \begin{lstlisting}[language=Python]
% API Conducer<C, I1, F1, I2, F2>
%     consume: (I1, C) -> C
%     close: (F1, C) -> ()
%     produce: P -> (I2, P) | F2
% \end{lstlisting}

% Setting \texttt{I1} and \texttt{F1} to $!$ yields an abstraction isomorphic to \texttt{Producer}, and setting \texttt{I2} and \texttt{F2} to $!$ yields an abstraction isomorphic to \texttt{Consumer}. We leave to the reader whether this constitutes an inconsequential triviality or an alternate lens through which our whole subject should be considered.

\subsection{Syntax Considerations}\label{syntax}

Many programming languages offer generator syntax for creating iterators, and for loops for consuming iterators. A language designed with our APIs in mind could provide more powerful syntax.

Generators\footnote{\url{https://peps.python.org/pep-0255/}} provide dedicated syntax for creating producers, with \texttt{yield} emitting repeated items and \texttt{return} emitting the final value. As an example, the following pseudo-code emits the numbers from zero to nine and then the final string ``hi''. We use atypical choices of keywords (\texttt{producer} instead of \texttt{generator}, \texttt{produce} instead of \texttt{yield}, and \texttt{produce final} instead of \texttt{return}) to be obnoxiously explicit about the intended semantics, and to prepare for a symmtric consumer design:

\begin{lstlisting}[language=Python]
producer
    i = 0
    while i < 10
        produce i
    produce final "hi"
\end{lstlisting}

We are not aware of any language that provides a symmetric construction for creating consumers. Dreaming up a symmetric design initially seems straightfoward enough:

\begin{lstlisting}[language=Python]
consumer
    x = consume
    y = consume
until consume final z
    doSomething(x + y + z)
\end{lstlisting}

This design does leave open some questions: what if the \texttt{consume} function of the created consumer is called more often then there are \texttt{consume} keywords in the main consumer body? And should it always be valid to jump to the \texttt{until consume final} block, or only at the end of the main consumer body?

Since the basic consumer design allows no communication to the calling code, a simple solution to the problem of too many \textit{consume} calls is to implicitly wrap the main consumer body in a loop. In a setting with fallible consumers, a consumer that wants to limit the number of possible calls to \textit{consume} can simply add an extra \texttt{consume} expression and throw from there:

\begin{lstlisting}[language=Python]
consumer
    x = consume
    y = consume
    _ = consume
    throw "too much information"
until consume final z
    doSomething(x + y + z)
\end{lstlisting}

To allow for control about what to do when \textit{close} is called depending on the current state of the consumer, the naïve \texttt{until consume final} can be replaced with a mechanism that mimics \texttt{try-catch} blocks:

\begin{lstlisting}[language=Python]
consumer
    consumeblock
        x = consume
    until _
        throw "too little information"
    consumeblock
        y = consume
    until z
        doSomething(x + y + z)
    consumeblock
        _ = consume
    until _
        throw "too much information"
\end{lstlisting}

The syntax is deliberately painful: we do not claim that these are the best design choices, we merely want to demonstrate that providing a meaningful and useful consumer syntax is indeed possible. And after extrapolating the logic that leads to our API designs, designing generators into languages without a corresponding consumer equivalent feels questionable.

A particular usecase we want to highlight for explicit (asynchronous) \texttt{consumer} syntax is that of implementing asynchronous parsers. Typically this involves writing a state-machine or otherwise putting a lot of manual work into ensuring a parser that can suspend its execution when reaching the temporary end of input and then resume once more input becomes available. The \texttt{consumer} syntax allows writing asynchronous parsers that look just like synchronous ones.

Assuming the questions around dedicated \texttt{consumer} syntax have been solved, the next logical step is to combine the \texttt{consumer} and \texttt{producer} keywords into a more powerful \texttt{conducer} language construct. As an example, we sketch an encoder conducer for 16-bit integers into 8-bit integers:

\begin{lstlisting}[language=Python]
conducer
    consumeblock
        x = consume
        produce x / 256
        produce x % 256
    until f
        produce final f
\end{lstlisting}

From such a construct, both the consumer adapter and the producer adapter can be generated. For the consumer adapter, the \texttt{consume} expressions provide the entry points to the state machine of the \textit{consume} function, and each \texttt{produce} expression translates to a \textit{consume} call of the wrapped consumer. For the producer adapter, the \texttt{produce} expressions provide the entry points to the state machine of the \textit{produce} function, and each \texttt{consume} expression translates to a \textit{produce} call of the wrapped producer.


Finally, we want to draw a parallel to coroutines\cite{moura2009revisiting}, as implemented, for example, in Lua\cite{ierusalimschy2006programming}. In (that particular brand of) coroutines, the \texttt{yield} expression in the coroutine implementation not only yields a value to the outside world, but it also evaluates to a value that is given as part of the expression that resumes the coroutine. We can see our conducer syntax as a generalization of this pattern. Coroutines tie incoming and outgoing communication to the same points in the coroutine, marked by \texttt{yield}. In fact, this is equivalent to naïve conducers restricted to maintaining a one-to-one correspondence between consumption and production. Our syntax allows arbitrarily splitting the communication. Hence, conducers generalize coroutines.

\subsection{Buffering and Bulk Processing}\label{bulk}

We now briefly discuss some questions of efficiency. While consumers and producers make for nice building blocks of programs because they are easy to reason about, it is inefficient in practice to process items one by one.

One problem of processing items one at a time is that performing side effects is often expensive, for example, when system calls are involved. Writing a file byte by byte with individual system calls is orders of magnitude slower than buffering bytes sequentially in memory and writing many bytes with a single system call.

An easy solution is to allow consumers to buffer items internally, leaving them the freedom to arbitrarily delay actual processing indefinitely to optimize for efficiency. When writing to a consumer in order to perform side-effects, the programmer needs a way to force the consumer to stop delaying, \textit{flush} its buffer, and actually trigger the effects:

\begin{lstlisting}[language=Python]
API BufferedConsumer<C, I, F>
    consume: (I, C) -> C
    close: (F, C) -> ()
    flush: C -> C
\end{lstlisting}

The buffered consumer with a \texttt{flush} function is a staple of real-world APIs. The analogous functionality for producers, however, is one we have never encountered. The opposite of \textit{flushing} as much data as possible \textit{out of} a buffer is \textit{slurping} as much data as possible \textit{into} a buffer.

\begin{lstlisting}[language=Python]
API BufferedProducer<P, I, F>
    produce: P -> (I, P) | F
    slurp: P -> P
\end{lstlisting}

Unlike flushing consumers, slurping producers does not immediately serve to trigger effectful production of items. Still, there are arguments in favor of a slurp function on producers that go beyond the consistency gains of maintaining symmetry (although, to be clear, those alone would already suffice in our opinion). Consider a producer that emits items from some effectful source which might stop working at any moment (i.e., a network connection). Slurping allows the programmer to pre-fetch data even though processing the available data might be time-consuming and not yet finished.

System calls are not the only reason for processing data in bulk. Simply copying consecutive bytes in memory from one location to another is significantly more efficient than copying each byte individually. Hence, many programming languages offer APIs for producing or consuming many items at a time by way of \textit{slices} (a pointer paired with the number of items stored consecutively in memory starting at the pointed-to address).

A typical example of such \textit{readers} (producers of many bytes simultaneously) and \textit{writers} (consumers of many bytes simultaneously) are the \texttt{Reader}\footnote{\url{https://pkg.go.dev/io\#Reader}} and \texttt{Writer}\footnote{\url{https://pkg.go.dev/io\#Writer}} abstractions of the Go language. To translate them into pseudo-types, we write \texttt{\&r[T]} for a slice of values of type \texttt{T} that may be read but not written, and \texttt{\&w[T]} for a slice of values of type \texttt{T} that may be written but not read. The Go APIs then translate to the following:

\begin{lstlisting}[language=Python]
API Reader<R, I, E>
    read: (R, &w[I]) -> (R, Nat) | E

API Writer<W, I, E>
    write: (W, &r[I]) -> (W, Nat) | E
\end{lstlisting}

The \texttt{read} function \textit{writes} (produces) some number of items into a slice, and returns how many items were written. The \texttt{write} function \textit{reads} (consumes) some number of items from a slice, and returns how many items were read. A return value of zero typically indicates the end of the sequence\footnote{In an synchronous setting, if no data is currently available but there might be more data in the future, the functions should block instead of returning zero. In an asynchronous setting, the functions should be parked to be resumed at a later point.}. We can easily generalize to arbitrary final values of some type $F$ by requiring the returned number to be non-zero and extending the return sum type by a third\footnote{Or a \textit{second} option, if we consider the error case as an effect.} option of type $F$.

Setting aside the interesting naming choices and the fact that most langages unnecessarily specialize the item type to that of 8-bit integers, these APIs display a perfect symmetry that APIs for operating on individual items usually lack.

It is tempting to think of readers and writers as \textit{generalizations} of producers and consumers respectively, but that viewpoint brings a problematic amount of freedom --- which parts should be generalized, and which parts should stay the same? Consider, for example, our restrictions to exclusively reading or writing from slices. This is more restrictive than allowing arbitrary access to the slices, and given the defaults of programming languages (no mainstream languages support write-only pointers), the default choice of many would be unrestricted access to the slices. The Rust community has had to put a lot of energy into  dealing with the consequences of such an oversight in its standard library\footnote{Rust allows for uninitialized memory, but \textit{reading} from unitialized memory is unsafe. See \url{https://github.com/rust-lang/rfcs/blob/master/text/2930-read-buf.md} and \url{https://blog.yoshuawuyts.com/uninit-read-write/} for details on how this affects its reader API.}.

Instead, we propose to think about readers and writers as optimization details: any \textit{read} must be equivalent to a series of zero or more calls to \textit{produce}, and any \textit{write} must be equivalent to a series of zero or more calls to \textit{consume}. This viewpoint precisely defines the semantics of the reader and writer APIs, and cleanly specifies answers to questions that might otherwise be non-obvious: may \textit{read} access the contents of the slice? No. What should \textit{read} or \textit{write} do when given an empty slice? Nothing. Is every (buffered) reader or writer a (buffered) producer or consumer respectively? Absolutely.

This last question is crucial: readers are subtypes of producers, and writers are subtypes of consumers. If you take away only one point from this essay, this is the one. Readers and writers stem file system abstractions, the duality of reading and writing to or from a file make their symmetry an obvious requirement. Streams and sinks trace back to iterators, which arose from traversal of (polymorphic) data structures, hence making the genericity of items an obvious requirement. If programming languages had routinely linked the two abstractions by a subtyping relation, we could have had fully symmetric, fully generic, unified APIs for decades. Instead, these abstractions have remained incomplete, and, consequently, interoperate badly.

One problem with the reader and writer APIs is that they do not compose very nicely: in order to move data from a reader to a writer, you need to use allocate an array into which to first copy the data via \textit{read}, and from which to then copy the data via \textit{write}. An alternate API choice without this problem is to \textit{expose} slices of \textit{internal} buffers instead of \textit{processing} slices of \textit{external} buffers:

\begin{lstlisting}[language=Python]
API BulkProducer<P, I, F>
    producer_slots: (P) -> &r[I] | F
    process_produced: (P, Nat) -> P
extends BufferedProducer<P, I, F>

API BulkConsumer<C, I, F>
    consumer_slots: (C) -> &w[I]
    process_consumed: (P, Nat) -> P
extends BufferedConsumer<C, I, F>
# To close, use the BufferedConsumer
# close function
\end{lstlisting}

The \textit{consumer\_slots} function provides a slice into an inner buffer of a \texttt{BulkConsumer}, into which the calling code can write. To trigger actual processing of the written items, the \textit{proces\_consumed} function notifies the consumer how many items were written and tasks it to consume them. The semantics of calling \textit{process\_consumed} with some argument $n$ must be those of calling \textit{consume} $n$ times, with the items written to the slice returned by \textit{consumer\_slots}. The \texttt{BulkProducer} API works analogously.

Whereas a writer API requires the data to be \textit{consumed} to be in an array, the bulk consumer is required to organize its \textit{internal buffer} as an array. In practice, things are most efficient if \textit{both} sides of the exchange store data consecutively in memory, so we don't expect this shift in responsibility to make a difference to anyone who uses bulk processing for efficiency reasons anyways.

Our APIs are more low-level than the traditional reader and writer APIs. The \textit{read} and \textit{write} functions --- we propose to call them \textit{bulk\_produce} and \textit{bulk\_consume} --- can easily be implemented as helper functions that take a slice and copy its contents into the slots exposed by the bulk API.

Given such \textit{bulk\_produce} and \textit{bulk\_consume} functions, there are now two semantically equivalent ways of piping a bulk producer into a bulk consumer: \textit{pipe\_bulk\_consume} uses the \textit{producer\_slots} of the producer as the slice argument to \textit{bulk\_consume} on the consumer, and \textit{pipe\_bulk\_produce} uses the \textit{consumer\_slots} of the consumer as the slice argument to \textit{bulk\_produce} on the producer. Neither of these requires allocation of an external buffer to facilitate the communication.

A final, interesting observation on this topic concerns memory safety. In a language with a concept of uninitialized memory that is acceptable to write to but not to read from, a bulk consumer is free to expose a (write-only) slice of uninitialized memory in its \textit{consumer\_slots} function. Whenever \textit{process\_consumed} is called thereafter, the consumer can assume that the memory has been initialized. If the calling code is faulty, this can lead to undefined behavior, making the \textit{process\_consumed} function \textit{unsafe} in the Rust sense of the word, i.e., it can trigger undefined behavior when its contract is not fulfilled. There is no such problem with the bulk producer API. Thankfully, the \textit{bulk\_consume} helper function fully insulates from this source of errors.

\section{Onward!}\label{fun}

This concludes our main arguments and designs. But there is still plenty of engineering and research left to be done.

What is up with conducers? Is the introduction of dedicated syntax really the best way of deriving consumer \textit{and} producer adapters from a single specification? Is there a nicer API design that captures the same degree of composability without requiring this split? If dedicated syntax is the way to go, should there be dedicated syntax for bulk producers, bulk consumers, and bulk conducers? What would it look like? What about vectored I/O\footnote{\url{https://en.wikipedia.org/wiki/Vectored_I/O}}?

Concerning the dedicated syntax, we took a lot of shortcuts, not least of all the absolutely horrible syntax for consumers. On the more formal side, what should be the proper --- say, denotational --- semantics of a \texttt{conducer} syntactic element be? Given such formal semantics, what is a translation of the syntax into ``normal'' syntactic components of equivalent semantics? Which ``normal'' constructs are particularly helpful --- coroutines, continuations? Can you elegantly avoid such fancy constructs altogether?

Is the fact that conducers generalize coroutines a coincidence, or do conducers deserve study as a control-flow mechanism in their own right? Coroutines are as expressive as one-shot continuations, but strictly less expressive than general continuations~\cite{moura2009revisiting}. Where do conducers fall in this spectrum?

What is up with the symmetry between producers and consumers? Is there a general, formal setting for expressing APIs with a general, precise notion of duality, in which producers and consumers are dual in a formal sense? Did we simply not find it yet, or is this impossible? For infinite, homogeneous sequences, producers and consumers are actually dual (and correspond nicely to coalgebra and algebra respectively). Why and where exactly do things go wrong once you add dedicated final elements or effects such as irrecoverable errors?

How far can we take our unsatisfying substitutes for proper duality --- symmetry and inverse-like composition? There is plenty of literature on proving iterators correct, see~\cite{bily2022compositional} for a recent example. How much of such literature carries over to consumers, and how much has to be redeveloped from scratch? This question should serve as a powerful motivation for finding a framing in which producers and consumers are fully dual. Similar thoughts apply to optimization techniques.

Regarding more direct concerns of software engineering, which adaptors or combinators should make up the standard toolbox for composing sequence APIs? Which algebraic laws must they fulfil? What is a good technique for implementing combinators only once and then automatically deriving bulk versions? Conducers provide a good framing for unary combinators, but what about other combinators (say, a binary concatenation combinator)?

Producers and consumers strictly limit the interaction with a sequence. Aside from optimization details such as functions for providing estimates of the minimum and maximum number of items that can still be processed, the most obvious extension of our APIs is that of random-access. Readers and writers originate from the Unix notion of \textit{files}, and \textit{seeking} in a file is a core concern of this perspective. What do good APIs for seeking look like? Support for infinity sequences mandates relative offsets rather than absolute indexing. Does this mean that all such generalizations amount to Turing-machine APIs with a movable read/write head? Should writing do overwrites exclusively, or is there design space for elastic bands that support insertion of new items in-between older items (as well as proper deletion)? Can and should these two modes be captured in the same API, or do they require separate abstractions? What does a lattice of (sub-) APIs look like that provides a more nuanced yet practically useful version of ``everything is a file''?

Another avenue for generalization is provided by the expressive power of the APIs. Our producers and consumers correspomnd to the ($\omega$-) regular languages. Are there elegant APIs that capture the context-free languages? If you squint a lot, (sets of) producer types look quite similar to left-regular grammars --- which should not be too surprising, given the relation with regular languages. What is the formal version of ``squinting a lot''? Does it have an inverse, and which APIs do you obtain by ``unsquinting'' the grammars in Chomsky normal form?

Yet another (arguably more practically relevant) generalization is from sequences to other graphs. What are appropriate APIs for consuming or producing trees? How do different traversal orders (breadth-first, depth-first, etc) factor into the API designs? What about APIs for exploring only a single path through a tree? Will there be a link between APIs for tree processing and grammars of context-free languages? How far can we take sensible APIs for traversing more complex graphs like DAGs or even arbitrary digraphs?

Finally, APIs with support for seeking in sequences or more complex graphs open up the question of \textit{who} performs the seeking. In a traditional file system API for seeking in and reading from a file, it is the user code that invokes the seeking. But consider instead a texteditor that feeds changes to a text buffer to some plugin. Here, the user code (i.e., the plugin) reads data again, but it does not control \textit{where} in the sequence the updates happen. The same kind of inverted seeking can happen for more complex data structures: a text editor might update a plugin about changes to a (higher-order) syntax tree, for example. We are not aware of any principled investigation into such APIs.

\section{Further Reading}

In this final section, we want to share some references that could be of interest to anyone wishing to pursue those open questions or to implement a library of sequence abstractions.

We have primarily presented our API designs by deriving them from first principles, instead of relating them to existing designs. While there are plenty of languages and libraries to choose from for documentation of existing APIs, there is much less available material on the reasoning behind those APIs. A notable exception are Oleg Kiselyov's \textit{iteratees}~\cite{kiselyov2012iteratees} and the resulting streamlined and well-documented \texttt{iterIO} Haskell library\footnote{\url{https://hackage.haskell.org/package/iterIO-0.2.2/docs/Data-IterIO.html}}. Their expressivity and rich algebraic structures are remarkable, as is the viewpoint of iteratees as communicating sequential processes. Yet, the design differs significantly from ours, the inherent asymmetry is striking: enumerators and iteratees are not at all dual. Particularly interesting is the notion of \texttt{Inum}s in the \texttt{iterIO} library: they fulfil the same role as our naïve conducers, while being completely asymmetric (and hence avoiding the problems that require us to move from naïve to actual conducers).

Kiselyov's treasure trove of a website\footnote{\url{https://okmij.org/ftp/}} contains several\footnote{\url{https://okmij.org/ftp/Haskell/Iteratee/index.html}} collections\footnote{\url{https://okmij.org/ftp/Streams.html}} of writing\footnote{\url{https://okmij.org/ftp/Scheme/enumerators-callcc.html}} that pertain to sequence APIs. The writing focuses almost exclusively on producers, with barely a word on consumers or any notions of symmetry or duality. We find it quite exciting to see such a different (and deep) take on the same material.

Functional reactive programming (FRP) is concerned with APIs for building systems on event streams, a good overview is given in~\cite{perez2016functional}. Whereas a sequence can be interpreted as a value evolving over discrete timesteps, FRP tackles the challenges of building abstractions (and efficient implementations) for values varying over a continuous notion of time. Discussion of FRP invariably turns to restricting the treatment of time to that of discrete event steps; this notion of FRP is all about what we called producers, discussing efficient implementation techniques, adapters, and combinators. A prominent example of this brand of FRP is the Elm language~\cite{czaplicki2013asynchronous}. \Cref{wtfjs} contains a dozen popular javascript libraries for such FRP. 

\url{http://www.litech.org/~jed/papers/jmatch-popl2006.pdf} Interruptible Iterators

\url{https://ora.ox.ac.uk/objects/uuid:87a8d92c-017c-4b3f-89a1-ac3056ad51d7/files/m4762057d0a14ed0f7db1ad98e560fa3f} The Essence of the Iterator Pattern

Iterators: Signs of Weakness in Object-Oriented Languages \url{https://dl.acm.org/doi/pdf/10.1145/165507.165514}

Design and Specification of Iterators Using the Swapping Paradigm \url{https://www.researchgate.net/profile/Bruce-Weide/publication/3187667_Design_and_Specification_of_Iterators_Using_the_Swapping_Paradigm/links/09e415117c907dd6da000000/Design-and-Specification-of-Iterators-Using-the-Swapping-Paradigm.pdf}

Synthesizing Iterators from Abstraction Functions \url{https://dspace.mit.edu/bitstream/handle/1721.1/87058/Jackson_Synthesizing%20iterators.pdf?sequence=1&isAllowed=y}

Segmented Iterators and Hierarchical Algorithms \url{https://lafstern.org/matt/segmented.pdf}

New Iterator Concepts \url{https://svn.osgeo.org/fdo/branches/3.4/Thirdparty/boost/libs/iterator/doc/new-iter-concepts.pdf}

coalgebra, coinduction, etc. bisumulation

for strict sequence typing: Strongly Typed Heterogeneous Collections \url{http://softlang.uni-koblenz.de/HList/paper.pdf}


session types

pull-stream paper

\url{http://jiangxi.cs.uwm.edu/publication/rebls2020.pdf} (good related work)

\url{https://arxiv.org/pdf/1612.06668.pdf} (good related work) Structured Asynchrony with Algebraic Effects
Asynchronous Effects DANEL AHMAN and MATIJA PRETNAR,

\section{Conclusion}\label{conclusion}

TODO

vision of unified, consistent APIs throughout a full language ecosystem. Getting things right is simpler than getting them wrong.

On a meta note, this essay constitutes evidence that you can share research results with broad applications to a wide range of programming practitioners, without assuming the kind of deep, intuitive familiarity with the Haskell standard library that requires years of practice to obtain. How did that ever become accepted practice in the first place?

\section{Appendix A: Javascript Libraries}\label{wtfjs}

This list of javaScript libraries for working with lazy sequences is intended to demonstrate that there is a clear need for a solid design that people can fall back to rather than reinventing ad-hoc wheels over and over. We list libraries with at least 200 stars on Github, as of February 2024, found by searching Gihub for ``stream'', ``observable'', and ``reactive''.

\begin{itemize}
    \item \url{https://github.com/staltz/xstream}
    \item \url{https://github.com/mafintosh/streamx}
    \item \url{https://github.com/getify/monio}
    \item \url{https://github.com/getify/asynquence}
    \item \url{https://github.com/cyclejs/cyclejs}
    \item \url{https://github.com/winterbe/streamjs}
    \item \url{https://github.com/winterbe/sequency}
    \item \url{https://github.com/pull-stream/pull-stream}
    \item \url{https://github.com/dionyziz/stream.js}
    \item \url{https://github.com/caolan/highland}
    \item \url{https://github.com/kefirjs/kefir}
    \item \url{https://github.com/baconjs/bacon.js}
    \item \url{https://github.com/cujojs/most}
    \item \url{https://github.com/callbag/callbag}
    \item \url{https://github.com/paldepind/flyd}
\end{itemize}
    
The following libraries do not explicitly define \textit{streams}, but they do work with \textit{observables}. Observables are an abstraction for values that (discretely) vary over time. For most intents and purposes, this is isomorphic to the notion of a stream.

\begin{itemize}
    \item \url{https://github.com/reactivex/rxjs}
    \item \url{https://github.com/tc39/proposal-observable}
    \item \url{https://github.com/zenparsing/zen-observable}
    \item \url{https://github.com/vobyjs/oby}
    \item \url{https://github.com/adamhaile/S}
    \item \url{https://github.com/luwes/sinuous}
    \item \url{https://github.com/mobxjs/mobx}
    \item \url{https://github.com/fynyky/reactor.js}
    \item \url{https://github.com/ds300/derivablejs}
    \item \url{https://github.com/elbywan/hyperactiv}
    \item \url{https://github.com/component/reactive}
    \item \url{https://github.com/mattbaker/Reactive.js}
\end{itemize}

All these libraries exist in addition to language-level or runtime-level APIs such as the following:

\begin{itemize}
    \item \href{https://nodejs.org/api/stream.html}{Node JS Streams}, and their evolution: \begin{itemize}
        \item \href{https://nodejs.org/docs/v0.1.100/api.html}{streams0}
        \item \href{https://nodejs.org/docs/v0.4.0/api/streams.html}{streams1}
        \item \href{https://nodejs.org/docs/v0.10.0/api/stream.html}{streams2}
        \item \href{https://nodejs.org/docs/v0.11.5/api/stream.html}{streams3}
    \end{itemize}
    \item \href{https://streams.spec.whatwg.org/}{WHATWG Streams}
    \item \href{https://tc39.es/ecma262/multipage/control-abstraction-objects.html#sec-%iteratorprototype%-object}{ECMAScript Iterator}
    \item \href{https://tc39.es/ecma262/multipage/control-abstraction-objects.html#sec-asynciteratorprototype}{ECMAScript AsyncIterator}
\end{itemize}

\bibliographystyle{alphaurl}
\bibliography{main}
\end{document}
